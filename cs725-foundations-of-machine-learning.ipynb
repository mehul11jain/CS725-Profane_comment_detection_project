{"cells":[{"metadata":{},"cell_type":"markdown","source":"## CS725: Foundations of Machine Learning Project\n-------"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Train and Test Data\n---"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\ntest_labels = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\ntest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ntrain = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM Model\n---\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport  numpy as np\nimport pandas as pd\nimport re\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing import sequence\nfrom keras.models import Model\nfrom keras.layers import LSTM, Dense, Dropout, Input, Embedding, Bidirectional, GlobalMaxPool1D\n\nembd_file='../input/glove840b300dtxt/glove.840B.300d.txt'\ntrain_file='../input/jigsaw-toxic-comment-classification-challenge/train.csv'\ntest_file='../input/jigsaw-toxic-comment-classification-challenge/test.csv'\nCLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\nmax_words_to_take=1000000\n\ntrain_data= pd.read_csv(train_file)\ntest_data= pd.read_csv(test_file)\n\n#x=test_data['comment_text'][500]\nytrain=train_data.values[:,2:]\n\ndef preprocess(x):\n    #text_to_word_sequence(x, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n    x=x.lower()\n    #x='''aksv asdjasd HS \"@#$^^''''$%_)(*& c    sds'''.split()\n    x= re.sub(r\"[^a-z0-9,*,!.'-@\\\"]\", \" \", x)\n    x= re.sub(r\"\\s{2,}\", \" \", x)\n    return x\n\n\nxtrain=train_data['comment_text'].map(lambda x: preprocess(x))\nxtest=test_data['comment_text'].map(lambda x: preprocess(x))\n\ntrain_comment=list(xtrain)\ntest_comment=list(xtest)\ntokenizer = Tokenizer(num_words=max_words_to_take)\n\n#Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n\ntokenizer.fit_on_texts(train_comment+test_comment)\ntrain_sequences = tokenizer.texts_to_sequences(train_comment)\ntest_sequences = tokenizer.texts_to_sequences(test_comment)\n\nword_index=tokenizer.word_index\nword_index_items=word_index.items()\nindex_to_word = {v: k for k, v in  word_index_items}\nprint(len(word_index))\n\naverage_length = np.mean([len(x) for x in train_sequences])#60\nmedian_length = sorted([len(x) for x in train_sequences])[len(train_sequences) // 2]#30\nmax_length= np.max([len(x) for x in train_sequences])#2142\n\nprint(\"Average sequence length: \", average_length)\nprint(\"Median sequence length: \", median_length)\n\nmax_sequence_length=150\n\nx_train = sequence.pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\nx_test = sequence.pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n\ny_train=ytrain\n\nprint('X_train shape: ', x_train.shape)\n\nembd_index = {}\n\nf = open(embd_file,encoding=\"utf8\")\nfor line in f:\n    values=line.split()\n    #word=values[0]\n    #vec=np.array(values[1:]).astype(np.float32)\n    word = ' '.join(values[:-300])\n    vec= np.asarray(values[-300:], dtype='float32')\n    #vec= vec.reshape(-1)\n    embd_index[word]=vec\nf.close\n\nembd_num_words = min(max_words_to_take, len(word_index) + 1)#0 not in word index\nembd_dim=300\n\n# from keras documentation\nembd_matrix= np.zeros((embd_num_words,embd_dim))\nfor word, i in  word_index_items:\n    if i > max_words_to_take :\n        continue\n    embd_vec=embd_index.get(word)\n    if embd_vec is not None:\n        #word not found in embedding index will be all-zeros\n        embd_matrix[i]=embd_vec\nprint('Null word embeddings: %d' % np.sum(np.sum(embd_matrix, axis=1) == 0))\n\n\ndef get_model():\n    hidden_size=200\n    xinput=Input(shape=(max_sequence_length,),dtype=\"int32\")\n    xembd=Embedding(embd_num_words,embd_dim, weights=[embd_matrix],input_length=max_sequence_length)(xinput)\n    xBlstm=Bidirectional(LSTM(hidden_size,return_sequences=True))(xembd)\n    xPool=GlobalMaxPool1D()(xBlstm)\n    \n    x=Dense(75,activation='relu')(xPool)\n    x=Dropout(0.1)(x)\n    predictions=Dense(6,activation='sigmoid')(x)# can be sigmoid\n    model=Model(inputs=xinput,outputs=predictions)\n    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])#crossentropy\n    return model\n\n    \n    \nmodel= get_model()\nbatch_size=256\nepochs=1\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfile_path=\"weights_bestEpoch.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n\n\ncallbacks_list = [checkpoint, early] #early\nhist=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n\nmodel.load_weights(file_path)\nbest_score = min(hist.history['val_loss'])\nprint(\"out of \",epochs,\"epochs best(minimum) validation-loss obtained is\",best_score )\n\n\ny_test_predict = model.predict(x_test,batch_size=1024,verbose=1)\n\n\n\npredict=pd.DataFrame(data=y_test_predict,columns=CLASSES)\n\n\ntest_ids=test_data[\"id\"].values\npredict[\"id\"]=test_ids.reshape((len(test_ids),1))\npredict=predict[[\"id\"]+CLASSES]\n\n\npredict.to_csv(\"sample_submission.csv\", index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2D-CNN\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import  numpy as np\nimport pandas as pd\nimport re\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing import sequence\nfrom keras.models import Model\nfrom keras.layers import concatenate, CuDNNGRU\n\n\nfrom keras.layers import Dense, Dropout, Input, Embedding,GlobalMaxPool1D\n\ntrain_file='data/train2.csv'\ntest_file='data/test.csv'\nembd_file='data/glove.840B.300d.txt'\nCLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nmax_words_to_take=1000000\nmax_sequence_length=200\n\ntrain_data= pd.read_csv(train_file)\ntest_data= pd.read_csv(test_file)\n\n#x=test_data['comment_text'][500]\nytrain=train_data.values[:,2:]\n\ndef preprocess(x):\n    #text_to_word_sequence(x, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n    x=x.lower()\n    #x='''aksv asdjasd HS \"@#$^^''''$%_)(*& c    sds'''.split()\n    x= re.sub(r\"[^a-z0-9,*,!.'-@\\\"]\", \" \", x)\n    x= re.sub(r\"\\s{2,}\", \" \", x)\n    return x\n\n\nxtrain=train_data['comment_text'].map(lambda x: preprocess(x))\nxtest=test_data['comment_text'].map(lambda x: preprocess(x))\n\ndef text_to_commentList(x):\n    return x\n\n\ntest_comment=list(xtrain)\ntrain_comment=list(xtest)\ntokenizer = Tokenizer(num_words=max_words_to_take)\n\n#Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n\ntokenizer.fit_on_texts(train_comment+test_comment)\ntrain_sequences = tokenizer.texts_to_sequences(train_comment)\ntest_sequences = tokenizer.texts_to_sequences(test_comment)\n\nword_index=tokenizer.word_index\nword_index_items=word_index.items()\nindex_to_word = {v: k for k, v in  word_index_items}\nprint(len(word_index))\n\naverage_length = np.mean([len(x) for x in train_sequences])#60\nmedian_length = sorted([len(x) for x in train_sequences])[len(train_sequences) // 2]#30\nmax_length= np.max([len(x) for x in train_sequences])#2142\n\nprint(\"Average sequence length: \", average_length)\nprint(\"Median sequence length: \", median_length)\n\n\n\nx_train = sequence.pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\nx_test = sequence.pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n\ny_train=ytrain\n\nprint('X_train shape: ', x_train.shape)\n\nembd_index = {}\n\nf = open(embd_file,encoding=\"utf8\")\nfor line in f:\n    values=line.split()\n    #word=values[0]\n    #vec=np.array(values[1:]).astype(np.float32)\n    word = ' '.join(values[:-300])\n    vec= np.asarray(values[-300:], dtype='float32')\n    #vec= vec.reshape(-1)\n    embd_index[word]=vec\nf.close\n\nembd_num_words = min(max_words_to_take, len(word_index) + 1)#0 not in word index\nembd_dim=300\n\n# from keras documentation\nembd_matrix= np.zeros((embd_num_words,embd_dim))\nfor word, i in  word_index_items:\n    if i > max_words_to_take :\n        continue\n    embd_vec=embd_index.get(word)\n    if embd_vec is not None:\n        #word not found in embedding index will be all-zeros\n        embd_matrix[i]=embd_vec\nprint('Null word embeddings: %d' % np.sum(np.sum(embd_matrix, axis=1) == 0))\n\n\ndef get_model():\n    #hidden_size=200\n    xinput=Input(shape=(max_sequence_length,),dtype=\"int32\")\n    xembd=Embedding(embd_num_words,embd_dim, weights=[embd_matrix],input_length=max_sequence_length)(xinput)\n    x1=SpatialDropout1D(0.4)(xembd)\n    x1= Reshape((200, 300, 1))(x1)\n    \n    filter_sizes=[1,2,3,5]\n    num_filters=32\n    pooled = []\n    for i in filter_sizes:\n        conv= Conv2D(num_filters, kernel_size=(i, 300), kernel_initializer='normal',\n                    activation='elu')(x1)\n        #maxpool_pre = MaxPool2D(pool_size=(maxlen - i + 1, 1))(conv_pre)\n        #avepool_pre = AveragePooling2D(pool_size=(maxlen - i + 1, 1))(conv_pre)\n        globalmax= GlobalMaxPooling2D()(conv_pre)\n        pooled.append(globalmax)\n    x1 = Concatenate(axis=1)(pooled)   \n    x1 = Dropout(0.2)(x1)\n\n    predictions=Dense(6,activation='sigmoid')(x1)# can be sigmoid\n    model=Model(inputs=xinput,outputs=predictions)\n    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])#crossentropy\n    return model\n    \n    \n    \nmodel= get_model()\nbatch_size=256\nepochs=1\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfile_path=\"weights_bestEpoch.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n\n\ncallbacks_list = [checkpoint, early] #early\nhist=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n\nmodel.load_weights(file_path)\nbest_score = min(hist.history['val_loss'])\nprint(\"out of \",epochs,\"epochs best(minimum) validation-loss obtained is\",best_score )\n\n\ny_test_predict = model.predict(xtrain,batch_size=1024,verbose=1)\n\n\n\npredict=pd.DataFrame(data=y_test_predict,columns=CLASSES)\n\n\ntest_ids=test_data[\"id\"].values\npredict[\"id\"]=test_ids.reshape((len(test_ids),1))\npredict=predict[[\"id\"]+CLASSES]\n\n\npredict.to_csv(\"sample_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\nfrom datetime import datetime\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n              (thour, tmin, round(tsec, 2)))\n\n# Data processing was done as in Bojan's fork of the original script:\n# https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams\n\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntraintime = timer(None)\ntrain_time = timer(None)\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv').fillna(' ')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv').fillna(' ')\ntr_ids = train[['id']]\ntrain[class_names] = train[class_names].astype(np.int8)\ntarget = train[class_names]\n\nprint(' Cleaning ...')\n# PREPROCESSING PART\nrepl = {\n    \"yay!\": \" good \",\n    \"yay\": \" good \",\n    \"yaay\": \" good \",\n    \"yaaay\": \" good \",\n    \"yaaaay\": \" good \",\n    \"yaaaaay\": \" good \",\n    \":/\": \" bad \",\n    \":&gt;\": \" sad \",\n    \":')\": \" sad \",\n    \":-(\": \" frown \",\n    \":(\": \" frown \",\n    \":s\": \" frown \",\n    \":-s\": \" frown \",\n    \"&lt;3\": \" heart \",\n    \":d\": \" smile \",\n    \":p\": \" smile \",\n    \":dd\": \" smile \",\n    \"8)\": \" smile \",\n    \":-)\": \" smile \",\n    \":)\": \" smile \",\n    \";)\": \" smile \",\n    \"(-:\": \" smile \",\n    \"(:\": \" smile \",\n    \":/\": \" worry \",\n    \":&gt;\": \" angry \",\n    \":')\": \" sad \",\n    \":-(\": \" sad \",\n    \":(\": \" sad \",\n    \":s\": \" sad \",\n    \":-s\": \" sad \",\n    r\"\\br\\b\": \"are\",\n    r\"\\bu\\b\": \"you\",\n    r\"\\bhaha\\b\": \"ha\",\n    r\"\\bhahaha\\b\": \"ha\",\n    r\"\\bdon't\\b\": \"do not\",\n    r\"\\bdoesn't\\b\": \"does not\",\n    r\"\\bdidn't\\b\": \"did not\",\n    r\"\\bhasn't\\b\": \"has not\",\n    r\"\\bhaven't\\b\": \"have not\",\n    r\"\\bhadn't\\b\": \"had not\",\n    r\"\\bwon't\\b\": \"will not\",\n    r\"\\bwouldn't\\b\": \"would not\",\n    r\"\\bcan't\\b\": \"can not\",\n    r\"\\bcannot\\b\": \"can not\",\n    r\"\\bi'm\\b\": \"i am\",\n    \"m\": \"am\",\n    \"r\": \"are\",\n    \"u\": \"you\",\n    \"haha\": \"ha\",\n    \"hahaha\": \"ha\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"can't\": \"can not\",\n    \"cannot\": \"can not\",\n    \"i'm\": \"i am\",\n    \"m\": \"am\",\n    \"i'll\" : \"i will\",\n    \"its\" : \"it is\",\n    \"it's\" : \"it is\",\n    \"'s\" : \" is\",\n    \"that's\" : \"that is\",\n    \"weren't\" : \"were not\",\n}\n\nkeys = [i for i in repl.keys()]\n\nnew_train_data = []\nnew_test_data = []\nltr = train[\"comment_text\"].tolist()\nlte = test[\"comment_text\"].tolist()\nfor i in ltr:\n    arr = str(i).split()\n    xx = \"\"\n    for j in arr:\n        j = str(j).lower()\n        if j[:4] == 'http' or j[:3] == 'www':\n            continue\n        if j in keys:\n            # print(\"inn\")\n            j = repl[j]\n        xx += j + \" \"\n    new_train_data.append(xx)\nfor i in lte:\n    arr = str(i).split()\n    xx = \"\"\n    for j in arr:\n        j = str(j).lower()\n        if j[:4] == 'http' or j[:3] == 'www':\n            continue\n        if j in keys:\n            # print(\"inn\")\n            j = repl[j]\n        xx += j + \" \"\n    new_test_data.append(xx)\ntrain[\"new_comment_text\"] = new_train_data\ntest[\"new_comment_text\"] = new_test_data\n\ntrate = train[\"new_comment_text\"].tolist()\ntete = test[\"new_comment_text\"].tolist()\nfor i, c in enumerate(trate):\n    trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\nfor i, c in enumerate(tete):\n    tete[i] = re.sub('[^a-zA-Z ?!]+', '', tete[i])\ntrain[\"comment_text\"] = trate\ntest[\"comment_text\"] = tete\ndel trate, tete\ntrain.drop([\"new_comment_text\"], axis=1, inplace=True)\ntest.drop([\"new_comment_text\"], axis=1, inplace=True)\n\ntrain_text = train['comment_text']\ntest_text = test['comment_text']\nall_text = pd.concat([train_text, test_text])\ntimer(train_time)\n\ntrain_time = timer(None)\nprint(' Part 1/2 of vectorizing ...')\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=10000)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)\ntimer(train_time)\n\ntrain_time = timer(None)\nprint(' Part 2/2 of vectorizing ...')\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2, 6),\n    max_features=50000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)\ntimer(train_time)\n\ntrain_features = hstack([train_char_features, train_word_features]).tocsr()\ntest_features = hstack([test_char_features, test_word_features]).tocsr()\ntimer(traintime)\n\nall_parameters = {\n                  'C'             : [1.048113, 0.1930, 0.596362, 0.25595, 0.449843, 0.25595],\n                  'tol'           : [0.1, 0.1, 0.046416, 0.0215443, 0.1, 0.01],\n                  'solver'        : ['lbfgs', 'newton-cg', 'lbfgs', 'newton-cg', 'newton-cg', 'lbfgs'],\n                  'fit_intercept' : [True, True, True, True, True, True],\n                  'penalty'       : ['l2', 'l2', 'l2', 'l2', 'l2', 'l2'],\n                  'class_weight'  : [None, 'balanced', 'balanced', 'balanced', 'balanced', 'balanced'],\n                 }\n\nfolds = 3\nscores = []\nscores_classes = np.zeros((len(class_names), folds))\n\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\nsubmission_oof = train[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n#skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1001)\nkf = KFold(n_splits=folds, shuffle=True, random_state=239)\n\nidpred = tr_ids\n\ntraintime = timer(None)\nfor j, (class_name) in enumerate(class_names):\n#    train_target = train[class_name]\n\n    classifier = LogisticRegression(\n        C=all_parameters['C'][j],\n        max_iter=200,\n        tol=all_parameters['tol'][j],\n        solver=all_parameters['solver'][j],\n        fit_intercept=all_parameters['fit_intercept'][j],\n        penalty=all_parameters['penalty'][j],\n        dual=False,\n        class_weight=all_parameters['class_weight'][j],\n        verbose=0)\n\n    avreal = target[class_name]\n    lr_cv_sum = 0\n    lr_pred = []\n    lr_fpred = []\n    lr_avpred = np.zeros(train.shape[0])\n\n    train_time = timer(None)\n    for i, (train_index, val_index) in enumerate(kf.split(train_features)):\n        X_train, X_val = train_features[train_index], train_features[val_index]\n        y_train, y_val = target.loc[train_index], target.loc[val_index]\n\n        classifier.fit(X_train, y_train[class_name])\n        scores_val = classifier.predict_proba(X_val)[:, 1]\n        lr_avpred[val_index] = scores_val\n        lr_y_pred = classifier.predict_proba(test_features)[:, 1]\n        scores_classes[j][i] = roc_auc_score(y_val[class_name], scores_val)\n        print('\\n Fold %02d class %s AUC: %.6f' % ((i+1), class_name, scores_classes[j][i]))\n\n        if i > 0:\n            lr_fpred = lr_pred + lr_y_pred\n        else:\n            lr_fpred = lr_y_pred\n\n        lr_pred = lr_fpred\n\n    lr_cv_score = (lr_cv_sum / folds)\n    lr_oof_auc = roc_auc_score(avreal, lr_avpred)\n    print('\\n Average class %s AUC:\\t%.6f' % (class_name, np.mean(scores_classes[j])))\n    print(' Out-of-fold class %s AUC:\\t%.6f' % (class_name, lr_oof_auc))\n    timer(train_time)\n\n    submission[class_name] = lr_pred / folds\n    submission_oof['prediction_' + class_name] = lr_avpred\n\nprint('\\n Overall AUC:\\t%.6f' % (np.mean(scores_classes)))\nsubmission.to_csv('submission-tuned-LR-01.csv', index=False)\nsubmission_oof.to_csv('oof-tuned-LR-01.csv', index=False)\ntimer(traintime)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}